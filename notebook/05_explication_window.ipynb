{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "import shap\n",
    "\n",
    "EXPLANATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"out_of_stock_product\",\n",
    "        \"recommended_product\",\n",
    "        \"key_similarities\",\n",
    "        \"ranking_reason\"\n",
    "    ],\n",
    "    template=\"\"\"\n",
    "Tu es un assistant e-commerce chargé d'expliquer une recommandation produit à un client.\n",
    "\n",
    "Produit initialement souhaité (en rupture) :\n",
    "- {out_of_stock_product}\n",
    "\n",
    "Produit recommandé en remplacement :\n",
    "- {recommended_product}\n",
    "\n",
    "Éléments de similarité clés :\n",
    "{key_similarities}\n",
    "\n",
    "Raisons principales du classement :\n",
    "{ranking_reason}\n",
    "\n",
    "Rédige une explication claire, courte et orientée client final :\n",
    "- Ton rassurant\n",
    "- Pas de jargon technique\n",
    "- Mettre en avant la valeur pour le client\n",
    "- 3 à 5 phrases maximum\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(ranker)\n",
    "shap_values = explainer.shap_values(X_topk) \n",
    "\n",
    "def get_top_shap_features(shap_values, feature_names, top_n=3):\n",
    "    \"\"\"\n",
    "    Retourne les top_n features avec impact positif sur le score\n",
    "    \"\"\"\n",
    "    top_features_list = []\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        # prendre les indices des plus grandes valeurs SHAP\n",
    "        top_idx = np.argsort(shap_values[i])[::-1][:top_n]\n",
    "        top_features = [(feature_names[j], shap_values[i][j]) for j in top_idx]\n",
    "        top_features_list.append(top_features)\n",
    "    return top_features_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dee6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # ou autre modèle\n",
    "    temperature=0.4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_replacement_explanation_shap(\n",
    "    out_of_stock_product: dict,\n",
    "    recommended_product: dict,\n",
    "    shap_features: list,\n",
    "    ranking_score: float\n",
    ") -> str:\n",
    "\n",
    "    # Construire une version lisible des features SHAP\n",
    "    key_similarities = []\n",
    "    for feat, val in shap_features:\n",
    "        # Interprétation simple côté client\n",
    "        if \"price\" in feat.lower():\n",
    "            key_similarities.append(f\"- Gamme de prix similaire\")\n",
    "        elif \"category\" in feat.lower():\n",
    "            key_similarities.append(f\"- Même catégorie\")\n",
    "        elif \"brand\" in feat.lower():\n",
    "            key_similarities.append(f\"- Même marque\")\n",
    "        else:\n",
    "            key_similarities.append(f\"- {feat} similaire ou favorable\")\n",
    "\n",
    "    ranking_reason = (\n",
    "        f\"Ce produit a obtenu un score élevé ({ranking_score:.2f}) \"\n",
    "        \"grâce à ses caractéristiques correspondant étroitement à vos préférences.\"\n",
    "    )\n",
    "\n",
    "    # Construire prompt\n",
    "    prompt = EXPLANATION_PROMPT.format(\n",
    "        out_of_stock_product=f\"{out_of_stock_product['name']} ({out_of_stock_product['brand']})\",\n",
    "        recommended_product=f\"{recommended_product['name']} ({recommended_product['brand']})\",\n",
    "        key_similarities=\"\\n\".join(key_similarities),\n",
    "        ranking_reason=ranking_reason\n",
    "    )\n",
    "\n",
    "    # Appel LLM\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987052ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_stock_product = {\n",
    "    \"name\": \"Casque Bluetooth X200\",\n",
    "    \"brand\": \"SoundMax\"\n",
    "}\n",
    "\n",
    "recommended_product = {\n",
    "    \"name\": \"Casque Bluetooth Pro X300\",\n",
    "    \"brand\": \"SoundMax\"\n",
    "}\n",
    "\n",
    "model_features = {\n",
    "    \"same_category\": \"Oui\",\n",
    "    \"price_similarity\": \"±5%\",\n",
    "    \"attribute_similarity\": \"Réduction de bruit, autonomie équivalente\"\n",
    "}\n",
    "\n",
    "ranking_score = 0.87  # score du LGBMRanker\n",
    "\n",
    "explanation = generate_replacement_explanation(\n",
    "    out_of_stock_product,\n",
    "    recommended_product,\n",
    "    model_features,\n",
    "    ranking_score\n",
    ")\n",
    "\n",
    "print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76837de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# 1Configuration LLM\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.4)\n",
    "CACHE_DIR = \"./explanation_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Prompts LangChain\n",
    "\n",
    "\n",
    "# Node 1 : transforme SHAP en langage client-friendly\n",
    "shap_prompt = PromptTemplate(\n",
    "    input_variables=[\"shap_features\"],\n",
    "    template=\"\"\"\n",
    "Tu es un assistant e-commerce. Transforme les features SHAP suivantes en langage client-friendly :\n",
    "{shap_features}\n",
    "\n",
    "Renvoie une liste de points clairs, simples et rassurants.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Node 2 : génère l’explication finale pour le client\n",
    "explanation_prompt = PromptTemplate(\n",
    "    input_variables=[\"out_of_stock_product\", \"recommended_product\", \"key_similarities\", \"ranking_score\"],\n",
    "    template=\"\"\"\n",
    "Produit en rupture : {out_of_stock_product}\n",
    "Produit recommandé : {recommended_product}\n",
    "Points clés : {key_similarities}\n",
    "Score : {ranking_score}\n",
    "\n",
    "Rédige une explication courte, rassurante et orientée client.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Fonction utilitaire SHAP\n",
    "\n",
    "def get_top_shap_features(shap_values, feature_names, top_n=3):\n",
    "    top_features_list = []\n",
    "    for i in range(shap_values.shape[0]):\n",
    "        top_idx = np.argsort(shap_values[i])[::-1][:top_n]\n",
    "        top_features = [(feature_names[j], shap_values[i][j]) for j in top_idx]\n",
    "        top_features_list.append(top_features)\n",
    "    return top_features_list\n",
    "\n",
    "\n",
    "# Fonction pour générer explication via LangChain\n",
    "\n",
    "def generate_explanation_chain(out_of_stock_product: str, recommended_product: str, shap_features: list, ranking_score: float):\n",
    "    \"\"\"\n",
    "    Génère l’explication client en deux étapes :\n",
    "    1️⃣ transformer SHAP en points clés\n",
    "    2️⃣ générer texte final\n",
    "    \"\"\"\n",
    "    # Cache key\n",
    "    cache_key = hashlib.md5(\n",
    "        json.dumps({\n",
    "            \"out\": out_of_stock_product,\n",
    "            \"rec\": recommended_product,\n",
    "            \"shap\": shap_features,\n",
    "            \"score\": ranking_score\n",
    "        }, sort_keys=True).encode()\n",
    "    ).hexdigest()\n",
    "    cache_file = os.path.join(CACHE_DIR, f\"{cache_key}.json\")\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, \"r\") as f:\n",
    "            return json.load(f)[\"explanation\"]\n",
    "\n",
    "    # --- Node 1 : SHAP -> points clés ---\n",
    "    shap_chain = LLMChain(llm=llm, prompt=shap_prompt, output_key=\"key_similarities\")\n",
    "    key_similarities = shap_chain.run(shap_features=shap_features)\n",
    "\n",
    "    # --- Node 2 : points clés -> explication finale ---\n",
    "    explanation_chain = LLMChain(llm=llm, prompt=explanation_prompt, output_key=\"explanation_text\")\n",
    "    explanation_text = explanation_chain.run(\n",
    "        out_of_stock_product=out_of_stock_product,\n",
    "        recommended_product=recommended_product,\n",
    "        key_similarities=key_similarities,\n",
    "        ranking_score=ranking_score\n",
    "    )\n",
    "\n",
    "    # Sauvegarde cache\n",
    "    with open(cache_file, \"w\") as f:\n",
    "        json.dump({\"explanation\": explanation_text}, f, ensure_ascii=False)\n",
    "\n",
    "    return explanation_text\n",
    "\n",
    "\n",
    "# Pipeline complet : Ranker → SHAP → LangChain\n",
    "\n",
    "def recommend_products_with_langchain(ranker, X_candidates: pd.DataFrame,\n",
    "                                      candidate_products: list,\n",
    "                                      out_of_stock_product: dict,\n",
    "                                      top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Retourne top-k produits + explications générées via LangChain\n",
    "    \"\"\"\n",
    "    # 1️⃣ Score top-k\n",
    "    scores = ranker.predict(X_candidates)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    top_products = [candidate_products[i] for i in top_idx]\n",
    "    top_scores = [scores[i] for i in top_idx]\n",
    "\n",
    "    # 2️⃣ SHAP\n",
    "    explainer = shap.TreeExplainer(ranker)\n",
    "    shap_values = explainer.shap_values(X_candidates)\n",
    "    top_shap_features_list = get_top_shap_features(shap_values, X_candidates.columns.tolist(), top_n=3)\n",
    "    top_shap_features = [top_shap_features_list[i] for i in top_idx]\n",
    "\n",
    "    # 3️⃣ LangChain pour explications\n",
    "    explanations = []\n",
    "    for prod, shap_feat, score in zip(top_products, top_shap_features, top_scores):\n",
    "        out_product_str = f\"{out_of_stock_product['name']} ({out_of_stock_product['brand']})\"\n",
    "        rec_product_str = f\"{prod['name']} ({prod['brand']})\"\n",
    "        explanation = generate_explanation_chain(out_product_str, rec_product_str, shap_feat, score)\n",
    "        explanations.append(explanation)\n",
    "\n",
    "    # 4️⃣ Retour\n",
    "    result = []\n",
    "    for prod, score, exp in zip(top_products, top_scores, explanations):\n",
    "        result.append({\n",
    "            \"product\": prod,\n",
    "            \"score\": score,\n",
    "            \"explanation\": exp\n",
    "        })\n",
    "\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
